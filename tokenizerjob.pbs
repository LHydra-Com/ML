#!/bin/bash
#PBS -N FlaxTokenizer100k
#PBS -q default
#PBS -l select=1:ncpus=16:mem=64gb
#PBS -l walltime=168:00:00
#PBS -m abe
#PBS -W group_list=x-ccast-prj-chayan

cd $PBS_O_WORKDIR

source ~/anaconda3/bin/activate flax

export HF_DATASETS_CACHE=$PWD/dataset

#python run_t5_mlm_flax.py \
#	--output_dir="./norwegian-t5-base" \
#	--model_type="t5" \
#	--config_name="./norwegian-t5-base" \
#	--tokenizer_name="./norwegian-t5-base" \
#	--dataset_name="oscar" \
#	--dataset_config_name="unshuffled_deduplicated_no" \
#	--max_seq_length="512" \
#	--per_device_train_batch_size="32" \
#	--per_device_eval_batch_size="32" \
#	--adafactor \
#	--learning_rate="0.005" \
#	--weight_decay="0.001" \
#	--warmup_steps="2000" \
#	--overwrite_output_dir \
#	--logging_steps="500" \
#	--save_steps="10000" \
#	--eval_steps="2500"

python3 train_tokenizer.py
